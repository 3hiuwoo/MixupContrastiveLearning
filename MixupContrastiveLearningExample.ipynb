{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IpTwQuz5p6K"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook provides code for mixup contrastive learning. The method is illustrated on the gunpoint dataset. The dataset used in this notebook is the gunpoint dataset. But more are available. See https://github.com/alan-turing-institute/sktime/tree/master/sktime/datasets/data for more info.\n",
        "\n",
        "The first two code block clones the sktime Github repo and loads the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5FKoYydvGLG",
        "outputId": "60ca183d-a527-48a9-e7a7-c9ff27e854a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sktime in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.24.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from sktime) (0.36.2)\n",
            "Requirement already satisfied: numba>=0.50 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.51.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.19.5)\n",
            "Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.12.2)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.1.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->sktime) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->sktime) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->sktime) (1.4.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sktime) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.50->sktime) (53.0.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->sktime) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->sktime) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5->statsmodels>=0.12.1->sktime) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Clone Git repos\n",
        "\n",
        "# !pip install sktime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mkzORFQhvWGQ"
      },
      "outputs": [],
      "source": [
        "#@title Load packages and data\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.signal import butter, lfilter\n",
        "from itertools import repeat\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from sktime.datasets import load_gunpoint\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "def to_np(x):\n",
        "    return x.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mvvnIdG6W8l"
      },
      "source": [
        "## Load data and create Pytorch dataset\n",
        "\n",
        "The following two code block loads the data, converts it to numpy array, before wrapping it in the Pytorch dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "KLEW5ZFz1R_x"
      },
      "outputs": [],
      "source": [
        "# #@title load data and convert to numpy array\n",
        "\n",
        "# x_tr, y_tr = load_gunpoint(split='train', return_X_y=True)\n",
        "\n",
        "# x_tr = pd.DataFrame(x_tr).to_numpy()\n",
        "# y_tr = pd.DataFrame(y_tr).to_numpy()\n",
        "\n",
        "# x_tr = np.array(np.ndarray.tolist(x_tr), dtype=np.float32)\n",
        "# y_tr = np.array(np.ndarray.tolist(y_tr), dtype=np.int32)\n",
        "\n",
        "# x_te, y_te = load_gunpoint(split='test', return_X_y=True)\n",
        "\n",
        "\n",
        "# x_te = pd.DataFrame(x_te).to_numpy()\n",
        "# y_te = pd.DataFrame(y_te).to_numpy()\n",
        "\n",
        "# x_te = np.array(np.ndarray.tolist(x_te), dtype=np.float32)\n",
        "# y_te = np.array(np.ndarray.tolist(y_te), dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5tNCbzc23edS"
      },
      "outputs": [],
      "source": [
        "# #@title create dataset\n",
        "\n",
        "# class MyDataset(Dataset):\n",
        "#     def __init__(self, x, y):\n",
        "\n",
        "#         device = 'cuda'\n",
        "#         self.x = torch.tensor(x, dtype=torch.float, device=device)\n",
        "#         self.y = torch.tensor(y, dtype=torch.long, device=device)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.x)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.x[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(root='dataset', name='chapman', length=None, overlap=0, norm=True):\n",
        "    ''' load and preprocess data\n",
        "    '''\n",
        "    data_path = os.path.join(root, name, 'feature')\n",
        "    labels, train_ids, valid_ids, test_ids = load_label_split(root, name)\n",
        "    \n",
        "    filenames = []\n",
        "    for fn in os.listdir(data_path):\n",
        "        filenames.append(fn)\n",
        "    filenames.sort()\n",
        "    \n",
        "    train_trials = []\n",
        "    train_labels = []\n",
        "    valid_trials = []\n",
        "    valid_labels = []\n",
        "    test_trials = []\n",
        "    test_labels = []\n",
        "    \n",
        "    for i, fn in enumerate(tqdm(filenames, desc=f'=> Loading {name}')):\n",
        "        label = labels[i]\n",
        "        feature = np.load(os.path.join(data_path, fn))\n",
        "        for trial in feature:\n",
        "            if i+1 in train_ids:\n",
        "                train_trials.append(trial)\n",
        "                train_labels.append(label)\n",
        "            elif i+1 in valid_ids:\n",
        "                valid_trials.append(trial)\n",
        "                valid_labels.append(label)\n",
        "            elif i+1 in test_ids:\n",
        "                test_trials.append(trial)\n",
        "                test_labels.append(label)\n",
        "                \n",
        "    X_train = np.array(train_trials)\n",
        "    X_val = np.array(valid_trials)\n",
        "    X_test = np.array(test_trials)\n",
        "    y_train = np.array(train_labels)\n",
        "    y_val = np.array(valid_labels)\n",
        "    y_test = np.array(test_labels)\n",
        "    \n",
        "    if norm:\n",
        "        X_train = process_batch_ts(X_train, normalized=True, bandpass_filter=False)\n",
        "        X_val = process_batch_ts(X_val, normalized=True, bandpass_filter=False)\n",
        "        X_test = process_batch_ts(X_test, normalized=True, bandpass_filter=False)\n",
        "      \n",
        "    if length:\n",
        "        # X_train, y_train = segment(X_train, y_train, split)\n",
        "        # X_val, y_val = segment(X_val, y_val, split)\n",
        "        # X_test, y_test = segment(X_test, y_test, split)\n",
        "        \n",
        "        X_train, y_train = split_data_label(X_train, y_train, sample_timestamps=length, overlapping=overlap)\n",
        "        X_val, y_val = split_data_label(X_val, y_val, sample_timestamps=length, overlapping=overlap)\n",
        "        X_test, y_test = split_data_label(X_test, y_test, sample_timestamps=length, overlapping=overlap)\n",
        "        \n",
        "    \n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "def load_label_split(root='dataset', name='chapman'):\n",
        "    '''\n",
        "    load labels for dataset and split information\n",
        "    '''\n",
        "    label_path = os.path.join(root, name, 'label', 'label.npy')\n",
        "    labels = np.load(label_path)\n",
        "    \n",
        "    if name == 'chapman':\n",
        "        pids_sb = list(labels[np.where(labels[:, 0]==0)][:, 1])\n",
        "        pids_af = list(labels[np.where(labels[:, 0]==1)][:, 1])\n",
        "        pids_gsvt = list(labels[np.where(labels[:, 0]==2)][:, 1])\n",
        "        pids_sr = list(labels[np.where(labels[:, 0]==3)][:, 1])\n",
        "        \n",
        "        train_ids = pids_sb[:-500] + pids_af[:-500] + pids_gsvt[:-500] + pids_sr[:-500]\n",
        "        val_ids = pids_sb[-500:-250] + pids_af[-500:-250] + pids_gsvt[-500:-250] + pids_sr[-500:-250]\n",
        "        test_ids = pids_sb[-250:] + pids_af[-250:] + pids_gsvt[-250:] + pids_sr[-250:]\n",
        "        \n",
        "    elif name == 'ptb':\n",
        "        pids_neg = list(labels[np.where(labels[:, 0]==0)][:, 1])\n",
        "        pids_pos = list(labels[np.where(labels[:, 0]==1)][:, 1])\n",
        "        \n",
        "        train_ids = pids_neg[:-14] + pids_pos[:-42]  # specify patient ID for training, validation, and test set\n",
        "        val_ids = pids_neg[-14:-7] + pids_pos[-42:-21]   # 28 patients, 7 healthy and 21 positive\n",
        "        test_ids = pids_neg[-7:] + pids_pos[-21:]  # # 28 patients, 7 healthy and 21 positive\n",
        "        \n",
        "    elif name == 'ptbxl':\n",
        "        pids_norm = list(labels[np.where(labels[:, 0]==0)][:, 1])\n",
        "        pids_mi = list(labels[np.where(labels[:, 0]==1)][:, 1])\n",
        "        pids_sttc = list(labels[np.where(labels[:, 0]==2)][:, 1])\n",
        "        pids_cd = list(labels[np.where(labels[:, 0]==3)][:, 1])\n",
        "        pids_hyp = list(labels[np.where(labels[:, 0]==3)][:, 1])\n",
        "        \n",
        "        train_ids = pids_norm[:-1200] + pids_mi[:-600] + pids_sttc[:-600] + pids_cd[:-400] + pids_hyp[:-200]\n",
        "        val_ids = pids_norm[-1200:-600] + pids_mi[-600:-300] + pids_sttc[-600:-300] + pids_cd[-400:-200] + pids_hyp[-200:-100]\n",
        "        test_ids = pids_norm[-600:] + pids_mi[-300:] + pids_sttc[-300:] + pids_cd[-200:] + pids_hyp[-100:]\n",
        "    \n",
        "    # TODO: CPSC2018, etc.\n",
        "    else:\n",
        "        raise ValueError(f'Unknown dataset: {name}')\n",
        "        \n",
        "    return labels, train_ids, val_ids, test_ids\n",
        "\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    ''' see https://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
        "\n",
        "    '''\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data, axis=0)\n",
        "    return y\n",
        "\n",
        "\n",
        "def process_ts(ts, fs, normalized=True, bandpass_filter=False):\n",
        "    ''' preprocess a time-series data\n",
        "\n",
        "    Args:\n",
        "        ts (numpy.ndarray): The input time-series in shape (timestamps, feature).\n",
        "        fs (float): The sampling frequency for bandpass filtering.\n",
        "        normalized (bool): Whether to normalize the time-series data.\n",
        "        bandpass_filter (bool): Whether to filter the time-series data.\n",
        "\n",
        "    Returns:\n",
        "        ts (numpy.ndarray): The processed time-series.\n",
        "    '''\n",
        "\n",
        "    if bandpass_filter:\n",
        "        ts = butter_bandpass_filter(ts, 0.5, 50, fs, 5)\n",
        "    if normalized:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(ts)\n",
        "        ts = scaler.transform(ts)\n",
        "    return ts\n",
        "\n",
        "\n",
        "def process_batch_ts(batch, fs=256, normalized=True, bandpass_filter=False):\n",
        "    ''' preprocess a batch of time-series data\n",
        "\n",
        "    Args:\n",
        "        batch (numpy.ndarray): A batch of input time-series in shape (n_samples, timestamps, feature).\n",
        "\n",
        "    Returns:\n",
        "        A batch of processed time-series.\n",
        "    '''\n",
        "\n",
        "    bool_iterator_1 = repeat(fs, len(batch))\n",
        "    bool_iterator_2 = repeat(normalized, len(batch))\n",
        "    bool_iterator_3 = repeat(bandpass_filter, len(batch))\n",
        "    return np.array(list(map(process_ts, batch, bool_iterator_1, bool_iterator_2, bool_iterator_3)))\n",
        "\n",
        "\n",
        "def split_data_label(X_trial, y_trial, sample_timestamps, overlapping):\n",
        "    ''' split a batch of time-series trials into samples and adding trial ids to the label array y\n",
        "\n",
        "    Args:\n",
        "        X_trial (numpy.ndarray): It should have a shape of (n_trials, trial_timestamps, features) B_trial x T_trial x C.\n",
        "        y_trial (numpy.ndarray): It should have a shape of (n_trials, 2). The first column is the label and the second column is patient id.\n",
        "        sample_timestamps (int): The length for sample-level data (T_sample).\n",
        "        overlapping (float): How many overlapping for each sample-level data in a trial.\n",
        "\n",
        "    Returns:\n",
        "        X_sample (numpy.ndarray): It should have a shape of (n_samples, sample_timestamps, features) B_sample x T_sample x C. The B_sample = B x sample_num.\n",
        "        y_sample (numpy.ndarray): It should have a shape of (n_samples, 3). The three columns are the label, patient id, and trial id.\n",
        "    '''\n",
        "    X_sample, trial_ids, sample_num = split_data(X_trial, sample_timestamps, overlapping)\n",
        "    # all samples from same trial should have same label and patient id\n",
        "    y_sample = np.repeat(y_trial, repeats=sample_num, axis=0)\n",
        "    # append trial ids. Segments split from same trial should have same trial ids\n",
        "    label_num = y_sample.shape[0]\n",
        "    y_sample = np.hstack((y_sample.reshape((label_num, -1)), trial_ids.reshape((label_num, -1))))\n",
        "    # X_sample, y_sample = shuffle(X_sample, y_sample)\n",
        "    return X_sample, y_sample\n",
        "\n",
        "\n",
        "def split_data(X_trial, sample_timestamps=256, overlapping=0.5):\n",
        "    ''' split a batch of trials into samples and mark their trial ids\n",
        "\n",
        "    Args:\n",
        "        See split_data_label() function\n",
        "\n",
        "    Returns:\n",
        "        X_sample (numpy.ndarray): (n_samples, sample_timestamps, feature).\n",
        "        trial_ids (numpy.ndarray): (n_samples,)\n",
        "        sample_num (int): one trial splits into sample_num of samples\n",
        "    '''\n",
        "    length = X_trial.shape[1]\n",
        "    # check if sub_length and overlapping compatible\n",
        "    if overlapping:\n",
        "        assert (length - (1-overlapping)*sample_timestamps) % (sample_timestamps*overlapping) == 0\n",
        "        sample_num = (length - (1 - overlapping) * sample_timestamps) / (sample_timestamps * overlapping)\n",
        "    else:\n",
        "        assert length % sample_timestamps == 0\n",
        "        sample_num = length / sample_timestamps\n",
        "    sample_feature_list = []\n",
        "    trial_id_list = []\n",
        "    trial_id = 1\n",
        "    for trial in X_trial:\n",
        "        counter = 0\n",
        "        # ex. split one trial(5s, 1280 timestamps) into 9 half-overlapping samples (1s, 256 timestamps)\n",
        "        while counter*sample_timestamps*(1-overlapping)+sample_timestamps <= trial.shape[0]:\n",
        "            sample_feature = trial[int(counter*sample_timestamps*(1-overlapping)):int(counter*sample_timestamps*(1-overlapping)+sample_timestamps)]\n",
        "            # print(f\"{int(counter*length*(1-overlapping))}:{int(counter*length*(1-overlapping)+length)}\")\n",
        "            sample_feature_list.append(sample_feature)\n",
        "            trial_id_list.append(trial_id)\n",
        "            counter += 1\n",
        "        trial_id += 1\n",
        "    X_sample, trial_ids = np.array(sample_feature_list), np.array(trial_id_list)\n",
        "\n",
        "    return X_sample, trial_ids, sample_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd4HHFN26mvJ"
      },
      "source": [
        "## Define neural network\n",
        "\n",
        "In this block we define the neural network architecture used. This architecture is based on the fully convolutional network from https://arxiv.org/abs/1611.06455, but with dilation added to each convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model.encoder import TSEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yL5kkfUHve_V"
      },
      "outputs": [],
      "source": [
        "#@title Define FCN\n",
        "\n",
        "class FCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCN, self).__init__()\n",
        "\n",
        "        self.encoder = TSEncoder(input_dims=12, output_dims=320)\n",
        "\n",
        "        self.proj_head = nn.Sequential(\n",
        "            nn.Linear(320, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h = self.encoder(x, pool='max')\n",
        "        out = self.proj_head(h)\n",
        "\n",
        "        return out, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUpO0DPU64zR"
      },
      "source": [
        "## Define loss, training function and evaluation function.\n",
        "\n",
        "The following three code blocks implements the mixup contrastive loss, the training function and the evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "igt1Rxsex4s-"
      },
      "outputs": [],
      "source": [
        "#@title define MixUp Loss\n",
        "\n",
        "class MixUpLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, device, batch_size):\n",
        "        super(MixUpLoss, self).__init__()\n",
        "        \n",
        "        self.tau = 0.5\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, z_aug, z_1, z_2, lam):\n",
        "\n",
        "        z_1 = nn.functional.normalize(z_1)\n",
        "        z_2 = nn.functional.normalize(z_2)\n",
        "        z_aug = nn.functional.normalize(z_aug)\n",
        "\n",
        "        labels_lam_0 = lam*torch.eye(self.batch_size, device=self.device)\n",
        "        labels_lam_1 = (1-lam)*torch.eye(self.batch_size, device=self.device)\n",
        "\n",
        "        labels = torch.cat((labels_lam_0, labels_lam_1), 1)\n",
        "\n",
        "        logits = torch.cat((torch.mm(z_aug, z_1.T),\n",
        "                         torch.mm(z_aug, z_2.T)), 1)\n",
        "\n",
        "        loss = self.cross_entropy(logits / self.tau, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def cross_entropy(self, logits, soft_targets):\n",
        "        return torch.mean(torch.sum(- soft_targets * self.logsoftmax(logits), 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Vppl1UTbx6xk"
      },
      "outputs": [],
      "source": [
        "#@title mixup model trainer per epoch\n",
        "\n",
        "\n",
        "def train_mixup_model_epoch(model, training_set, test_set, optimizer, alpha, epochs):\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    batch_size_tr = len(training_set.x)\n",
        "\n",
        "    LossList, AccList = [], []\n",
        "    criterion = MixUpLoss(device, batch_size_tr)\n",
        "\n",
        "    training_generator = DataLoader(training_set, batch_size=batch_size_tr,\n",
        "                                    shuffle=True, drop_last=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        for x, y in training_generator:\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_1 = x\n",
        "            x_2 = x[torch.randperm(len(x))]\n",
        "\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "            x_aug = lam * x_1 + (1-lam) * x_2\n",
        "\n",
        "            z_1, _ = model(x_1)\n",
        "            z_2, _ = model(x_2)\n",
        "            z_aug, _ = model(x_aug)\n",
        "\n",
        "            loss= criterion(z_aug, z_1, z_2, lam)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            LossList.append(loss.item())\n",
        "\n",
        "\n",
        "        AccList.append(test_model(model, training_set, test_set))\n",
        "\n",
        "        print(f\"Epoch number: {epoch}\")\n",
        "        print(f\"Loss: {LossList[-1]}\")\n",
        "        print(f\"Accuracy: {AccList[-1]}\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        torch.save(model.encoder.state_dict(), f'model_{epoch}.pth')\n",
        "        if epoch % 10 == 0 and epoch != 0: clear_output()\n",
        "            \n",
        "    return LossList, AccList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SSwqzgbEx8-B"
      },
      "outputs": [],
      "source": [
        "#@title model evaluation\n",
        "\n",
        "\n",
        "def test_model(model, training_set, test_set):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    N_tr = len(training_set.x)\n",
        "    N_te = len(test_set.x)\n",
        "\n",
        "    training_generator = DataLoader(training_set, batch_size=1,\n",
        "                                    shuffle=True, drop_last=False)\n",
        "    test_generator = DataLoader(test_set, batch_size= 1,\n",
        "                                    shuffle=True, drop_last=False)\n",
        "\n",
        "    H_tr = torch.zeros((N_tr, 128))\n",
        "    y_tr = torch.zeros((N_tr), dtype=torch.long)\n",
        "\n",
        "    H_te = torch.zeros((N_te, 128))\n",
        "    y_te = torch.zeros((N_te), dtype=torch.long)\n",
        "\n",
        "    for idx_tr, (x_tr, y_tr_i) in enumerate(training_generator):\n",
        "        with torch.no_grad():\n",
        "            _, H_tr_i = model(x_tr)\n",
        "            H_tr[idx_tr] = H_tr_i\n",
        "            y_tr[idx_tr] = y_tr_i\n",
        "\n",
        "    H_tr = to_np(nn.functional.normalize(H_tr))\n",
        "    y_tr = to_np(y_tr)\n",
        "\n",
        "\n",
        "    for idx_te, (x_te, y_te_i) in enumerate(test_generator):\n",
        "        with torch.no_grad():\n",
        "            _, H_te_i = model(x_te)\n",
        "            H_te[idx_te] = H_te_i\n",
        "            y_te[idx_te] = y_te_i\n",
        "\n",
        "    H_te = to_np(nn.functional.normalize(H_te))\n",
        "    y_te = to_np(y_te)\n",
        "\n",
        "    clf = KNeighborsClassifier(n_neighbors=1).fit(H_tr, y_tr)\n",
        "\n",
        "    return clf.score(H_te, y_te)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux9UVzA97GNi"
      },
      "source": [
        "## Block for training the model\n",
        "\n",
        "This block trains the neural network using mixup contrastive learning. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "227XYyoHx--y"
      },
      "outputs": [],
      "source": [
        "#@title Experiment number of epochs\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 100\n",
        "\n",
        "alpha = 1.0\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = load_data(root='/root/auto-tmp/dataset', name='ptbxl', length=300)\n",
        "training_set = torch.utils.data.TensorDataset(torch.from_numpy(X_train).to(torch.float32), torch.from_numpy(y_train).to(torch.long))\n",
        "test_set = torch.utils.data.TensorDataset(torch.from_numpy(X_val).to(torch.float32), torch.from_numpy(y_val).to(torch.long))\n",
        "\n",
        "model = FCN().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "LossListM, AccListM = train_mixup_model_epoch(model, training_set, test_set,\n",
        "                                              optimizer, alpha, epochs)\n",
        "\n",
        "\n",
        "print(f\"Score for alpha = {alpha}: {AccListM[-1]}\")\n",
        "\n",
        "\n",
        "plt.figure(1, figsize=(8, 8))\n",
        "plt.subplot(121)\n",
        "plt.plot(LossListM)\n",
        "plt.title('Loss')\n",
        "plt.subplot(122)\n",
        "plt.plot(AccListM)\n",
        "plt.title('Accuracy')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EtASHNL5nRU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MixupContrastiveLearningExample.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
